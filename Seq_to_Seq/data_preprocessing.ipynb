{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up working directory information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getcwd()\n",
    "data_directory = os.path.join(working_directory, 'data')\n",
    "models_directory = os.path.join(working_directory, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(os.path.join(data_directory, 'movie_lines.txt'), encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conversations = open(os.path.join(data_directory, 'movie_conversations.txt'), encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "word2vec_file = os.path.join(data_directory, \"glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_separator = \" +++$+++ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize top five lines\n",
    "lines[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ids to the line said by character\n",
    "id_to_line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(data_separator)\n",
    "    if len(_line) == 5:\n",
    "        id_to_line[_line[0]] = _line[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize top 5 conversations\n",
    "conversations[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_characters = r\"['\\s\\[\\]]\"\n",
    "conversations_ids = [re.sub(junk_characters, \"\", conv.split(data_separator)[-1]).split(\",\") for  conv in conversations[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L194', 'L195', 'L196', 'L197']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Questions and Answers data\n",
    "In conversations_ids, the list represents [Q, A, Q, A...] - Use this structure to create QnA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in conversations_ids:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id_to_line[conv[i]])\n",
    "        answers.append(id_to_line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common contractions\n",
    "common_contractions = {\n",
    "    r\"i'm\": \"i am\",\n",
    "    r\"he's\": \"he is\",\n",
    "    r\"she's\": \"she is\",\n",
    "    r\"that's\": \"that is\",\n",
    "    r\"what's\": \"what is\",\n",
    "    r\"where's\": \"where is\",\n",
    "    r\"\\'ll\": \" will\",\n",
    "    r\"\\'ve\": \" have\",\n",
    "    r\"\\'re\": \" are\",\n",
    "    r\"\\'d\": \" would\",\n",
    "    r\"won't\": \"will not\",\n",
    "    r\"can't\": \"can not\",\n",
    "    r\"n't\": \" not\",\n",
    "    r\"&\": \"and\",\n",
    "    r\"it's\": \"it is\",\n",
    "    r\"how's\": \"how is\",\n",
    "    r\"[$()\\\"#/@;:<>{}+=-`|.?,\\'*%_\\[\\]]|(-)+\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # replace common contractions\n",
    "    for contraction, replacement in common_contractions.items():\n",
    "        text = re.sub(contraction, replacement, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_questions = [clean_text(q) for q in questions]\n",
    "cleaned_answers = [f\"<SOS> {clean_text(a)} <EOS>\" for a in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS> well i thought we would start with pronunciation if that is okay with you <EOS>',\n",
       " '<SOS> not the hacking and gagging and spitting part  please <EOS>',\n",
       " '<SOS> okay then how bout we try out some french cuisine  saturday  night <EOS>',\n",
       " '<SOS> forget it <EOS>',\n",
       " '<SOS> cameron <EOS>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_answers[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other processing - Tokenization, padding, and converting to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(data, cap=512):\n",
    "    return min(max([len(t.split()) for t in data]), cap)\n",
    "\n",
    "def create_tokenizer(data, vocab_size=5000):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    word_idx_map = tokenizer.word_index\n",
    "    idx_word_map = {v:k for k,v in word_idx_map.items()}\n",
    "    return tokenizer, word_idx_map, idx_word_map\n",
    "\n",
    "def tokenize_sentences_and_pad(data, tokenizer, max_len):\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    tokenized_sents = tokenizer.texts_to_sequences(data)\n",
    "    return pad_sequences(tokenized_sents, padding='post', maxlen=max_len)\n",
    "\n",
    "\n",
    "def w2v_create_embeddings_matrix(embeddings_file, word_index_mapping, emb_dim=100):\n",
    "    import numpy as np\n",
    "    vocab_size = len(word_index_mapping) + 1  # Adding 1, as 0 index is reserved for OOV/UNK token\n",
    "    embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    with open(embeddings_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index_mapping:\n",
    "                idx = word_index_mapping[word]\n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:emb_dim]\n",
    "    return embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = get_max_len(cleaned_questions+cleaned_answers)\n",
    "tokenizer, word_idx_map, idx_word_map = create_tokenizer(cleaned_questions+cleaned_answers, vocab_size=20000)\n",
    "questions_tokenized = tokenize_sentences_and_pad(cleaned_questions, tokenizer, MAX_LEN)\n",
    "answers_tokenized = tokenize_sentences_and_pad(cleaned_answers, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix, _ = w2v_create_embeddings_matrix(word2vec_file, word_idx_map, emb_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64841"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  33   20  101 ...    0    0    0]\n",
      " [  58    4  134 ...    0    0    0]\n",
      " [   8    5 8938 ...    0    0    0]\n",
      " [   3   15  557 ...    0    0    0]\n",
      " [  30   30   10 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(questions_tokenized[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1   58    4 ...    0    0    0]\n",
      " [   1    8    5 ...    0    0    0]\n",
      " [   1  105   86 ...    0    0    0]\n",
      " [   1  326   10 ...    0    0    0]\n",
      " [   1 5695    2 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(answers_tokenized[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving outputs to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/ss_0002/Documents/work/other-repos/chatbot/Seq_to_Seq/data/answers_tokenized.h5']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(tokenizer, os.path.join(models_directory, \"tokenizer.h5\"))\n",
    "dump(word_idx_map, os.path.join(models_directory, \"word_idx_map.h5\"))\n",
    "dump(idx_word_map, os.path.join(models_directory, \"idx_word_map.h5\"))\n",
    "dump(embeddings_matrix, os.path.join(models_directory, \"embeddings_matrix.h5\"))\n",
    "dump(questions_tokenized, os.path.join(data_directory, \"questions_tokenized.h5\"))\n",
    "dump(answers_tokenized, os.path.join(data_directory, \"answers_tokenized.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
